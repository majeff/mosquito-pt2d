# Copyright 2025 Arduino PT2D Project
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
èšŠå­å½±åƒè­˜åˆ¥æ¨¡çµ„
ä½¿ç”¨æ·±åº¦å­¸ç¿’AIæ¨¡å‹ï¼ˆYOLOï¼‰åµæ¸¬èšŠå­
æ”¯æ´ç¡¬é«”åŠ é€Ÿæ¨ç†å¼•æ“ï¼šhobot_dnn (BPU) / rknnlite (NPU)
æ³¨æ„ï¼šONNX/PyTorch åƒ…ç”¨æ–¼è¨“ç·´å’Œæ¨¡å‹è½‰æ›ï¼Œä¸ç”¨æ–¼å¯¦éš›åµæ¸¬
"""

import cv2
import numpy as np
from typing import Optional, List, Tuple, Dict
import logging
import os
import time
import datetime
import hashlib
import traceback
from pathlib import Path
from config_loader import config

# ä»æ–°é…ç½®ä¸­è·å–é»˜è®¤å€¼
DEFAULT_IMGSZ = config.imgsz
DEFAULT_CONFIDENCE_THRESHOLD = config.confidence_threshold
DEFAULT_IOU_THRESHOLD = config.iou_threshold
DEFAULT_DETECTION_MODE = config.detection_mode
DEFAULT_TILE_OVERLAP = config.tile_overlap
DEFAULT_DETECTION_MARGIN = config.detection_margin
DEFAULT_MAX_SAMPLES = config.max_samples
DEFAULT_SAVE_INTERVAL = config.save_interval
DEFAULT_SAVE_HIGH_CONFIDENCE_SAMPLES = config.save_high_confidence_samples
SAMPLE_COLLECTION_DIR = config.sample_collection_dir
MEDIUM_CONFIDENCE_DIR = config.medium_confidence_dir
HIGH_CONFIDENCE_DIR = config.high_confidence_dir
ENABLE_ILLUMINATION_MONITORING = config.enable_illumination_monitoring
ILLUMINATION_WARNING_THRESHOLD = config.illumination_warning_threshold
ILLUMINATION_PAUSE_THRESHOLD = config.illumination_pause_threshold
ILLUMINATION_CHECK_INTERVAL = config.illumination_check_interval

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å˜—è©¦å°å…¥å„ç¨®æ¨ç†å¼•æ“
try:
    from rknnlite.api import RKNNLite
    RKNN_AVAILABLE = True
except ImportError:
    RKNN_AVAILABLE = False
    logger.debug("rknnlite æœªå®‰è£ï¼ˆåƒ… Orange Pi 5 éœ€è¦ï¼‰")

try:
    from hobot_dnn import pyeasy_dnn as dnn
    HOBOT_DNN_AVAILABLE = True
except ImportError:
    HOBOT_DNN_AVAILABLE = False
    logger.debug("hobot_dnn æœªå®‰è£ï¼ˆåƒ… RDK X5 éœ€è¦ï¼‰")

# ONNX å’Œ PyTorch åƒ…ç”¨æ–¼è¨“ç·´å’Œæ¨¡å‹è½‰æ›ï¼Œä¸ç”¨æ–¼å¯¦éš›åµæ¸¬
# å¯¦éš›éƒ¨ç½²æ™‚è«‹ä½¿ç”¨ç¡¬é«”åŠ é€Ÿæ ¼å¼ï¼š.bin (RDK X5) æˆ– .rknn (Orange Pi 5)


class MosquitoDetector:
    """åŸºæ–¼AIæ·±åº¦å­¸ç¿’çš„èšŠå­åµæ¸¬å™¨é¡ï¼ˆæ”¯æ´ RKNN/ONNX/PyTorchï¼‰"""

    def __init__(self,
                 model_path: Optional[str] = None,
                 confidence_threshold: float = DEFAULT_CONFIDENCE_THRESHOLD,
                 iou_threshold: float = DEFAULT_IOU_THRESHOLD,
                 imgsz: int = DEFAULT_IMGSZ,
                 detection_mode: str = DEFAULT_DETECTION_MODE,  # 'tiling' æˆ– 'whole'
                 tile_overlap: float = DEFAULT_TILE_OVERLAP,
                 detection_margin: float = DEFAULT_DETECTION_MARGIN,
                 fallback_to_pretrained: bool = True,
                 save_uncertain_samples: bool = False,
                 uncertain_conf_range: Tuple[float, float] = (0.4, 0.7),
                 save_dir: str = None,
                 max_samples: int = DEFAULT_MAX_SAMPLES,
                 save_interval: float = DEFAULT_SAVE_INTERVAL,
                 save_annotations: bool = True,
                 save_full_frame: bool = False):
        """
        åˆå§‹åŒ–AIèšŠå­åµæ¸¬å™¨ (ç¡¬é«”åŠ é€Ÿå°ˆç”¨)
        è‡ªå‹•é¸æ“‡ç¡¬é«”åŠ é€Ÿæ¨ç†å¼•æ“ï¼š
        - RDK X5: .bin (hobot_dnn BPU)
        - Orange Pi 5: .rknn (rknnlite NPU)

        Args:
            model_path: æ¨¡å‹è·¯å¾‘ï¼ˆå¯ä¸å«å‰¯æª”åï¼‰ï¼Œæˆ– models/ ç›®éŒ„ä¸‹çš„åŸºæœ¬åç¨±
                       ä¾‹å¦‚: "mosquito_yolov8" æœƒè‡ªå‹•æœå°‹
                       mosquito_yolov8.bin â†’ mosquito_yolov8.rknn
            confidence_threshold: ä¿¡å¿ƒåº¦é–¾å€¼ï¼ˆ0-1ï¼‰ï¼Œé è¨­ 0.4ï¼ˆæ¨è–¦ç¯„åœ 0.3-0.7ï¼‰
            iou_threshold: IoUé–¾å€¼ï¼ˆç”¨æ–¼NMSï¼‰ï¼Œé è¨­ 0.45
            imgsz: è¼¸å…¥å½±åƒå¤§å°ï¼Œé è¨­ 640ï¼ˆæ¨è–¦å€¼ï¼‰
                   - 320: å¿«é€Ÿæ¨ç†ï¼Œé©åˆä½æ•ˆèƒ½è¨­å‚™
                   - 640: æ¨™æº–ç²¾åº¦ï¼Œæ¨è–¦ä½¿ç”¨ï¼ˆOrange Pi 5 å¯æµæš¢é‹è¡Œï¼‰
            detection_margin: æª¢æ¸¬é‚Šç•Œé‚Šè·ï¼ˆ0.0-0.5ï¼‰ï¼Œæ’é™¤é‚Šç·£å€åŸŸï¼Œé è¨­ 0.1
            fallback_to_pretrained: å¦‚æœæ‰¾ä¸åˆ°è‡ªå®šç¾©æ¨¡å‹ï¼Œæ˜¯å¦ä½¿ç”¨é è¨“ç·´æ¨¡å‹
            save_uncertain_samples: æ˜¯å¦å„²å­˜ä¿¡å¿ƒåº¦ä¸­ç­‰çš„æ¨£æœ¬åœ–ç‰‡ä»¥ä¾¿å¾ŒçºŒæª¢é©—èˆ‡å†è¨“ç·´
            uncertain_conf_range: ä¸­ç­‰ä¿¡å¿ƒåº¦ç¯„åœ (min, max)ï¼Œé è¨­ (0.4, 0.7)
            save_dir: å„²å­˜ä¸­ç­‰ä¿¡å¿ƒåº¦æ¨£æœ¬çš„ç›®éŒ„
            max_samples: æœ€å¤§å­˜å„²ç…§ç‰‡æ•¸é‡ï¼Œé è¨­ 1000 å¼µ
            save_interval: å„²å­˜æ™‚é–“é–“éš”ï¼ˆç§’ï¼‰ï¼Œé¿å…é »ç¹å­˜åŒä¸€ä½ç½®çš„ç…§ç‰‡ï¼Œé è¨­ 3.0 ç§’
            save_annotations: æ˜¯å¦è‡ªå‹•ç”Ÿæˆ YOLO æ ¼å¼æ¨™è¨»æ–‡ä»¶ (.txt)ï¼Œé è¨­ True
            save_full_frame: æ˜¯å¦å„²å­˜å®Œæ•´ç•«é¢ï¼ˆTrueï¼‰æˆ–åƒ…è£å‰ªæª¢æ¸¬å€åŸŸï¼ˆFalseï¼‰ï¼Œé è¨­ False
        """
        self.confidence_threshold = confidence_threshold
        self.iou_threshold = iou_threshold
        self.imgsz = imgsz
        self.model = None
        self.backend = None  # 'rknn', 'onnx', 'pytorch'

        # åµæ¸¬æ¨¡å¼
        self.detection_mode = detection_mode.lower() if isinstance(detection_mode, str) else 'tiling'
        if self.detection_mode not in ('tiling', 'whole'):
            logger.warning(f"æœªçŸ¥çš„åµæ¸¬æ¨¡å¼: {self.detection_mode}ï¼Œæ”¹ç”¨ 'tiling'")
            self.detection_mode = 'tiling'
        # å¹³é‹ªé‡ç–Šæ¯”ä¾‹
        try:
            self.tile_overlap = float(tile_overlap)
        except Exception:
            self.tile_overlap = DEFAULT_TILE_OVERLAP
        self.tile_overlap = max(0.0, min(0.5, self.tile_overlap))

        # æª¢æ¸¬é‚Šç•Œé‚Šè·
        try:
            self.detection_margin = float(detection_margin)
        except Exception:
            self.detection_margin = DEFAULT_DETECTION_MARGIN
        self.detection_margin = max(0.0, min(0.5, self.detection_margin))

        # å„²å­˜åŠŸèƒ½é…ç½®
        self.save_uncertain_samples = save_uncertain_samples
        self.uncertain_conf_range = uncertain_conf_range
        # æ¨£æœ¬æ”¶é›†æ ¹ç›®éŒ„ï¼ˆèˆ‡ label_samples.py ä¿æŒä¸€è‡´ï¼‰
        self.collection_root = Path(config.sample_collection_dir)
        self.save_dir = self.collection_root  # å…¼å®¹èˆŠä»£ç¢¼ç”¨èª
        self.max_samples = config.max_samples
        self.save_interval = config.save_interval
        self.save_annotations = config.save_annotations
        self.save_full_frame = config.save_full_frame
        self.save_counter = 0
        self.last_save_time = 0.0  # ä¸Šæ¬¡å„²å­˜æ™‚é–“æˆ³
        self.last_saved_hash = None  # ä¸Šæ¬¡å„²å­˜ç…§ç‰‡çš„é›œæ¹Šå€¼

        # é«˜ä¿¡å¿ƒåº¦æ¨£æœ¬ä¿å­˜è¨­å®šï¼ˆé–¾å€¼ç›´æ¥å–ä¸­ä¿¡å¿ƒåº¦ä¸Šé™ï¼Œé¿å…è¨­å®šä¸ä¸€è‡´ï¼‰
        self.save_high_confidence = config.save_high_confidence_samples
        self.high_conf_threshold = float(self.uncertain_conf_range[1])

        # å‰µå»ºå„²å­˜ç›®éŒ„ï¼ˆèˆ‡æ¨™è¨»æµç¨‹ä¸€è‡´çš„å›ºå®šç›®éŒ„ï¼‰
        if self.save_uncertain_samples or self.save_high_confidence:
            self.collection_root.mkdir(parents=True, exist_ok=True, mode=0o755)
            Path(MEDIUM_CONFIDENCE_DIR).mkdir(parents=True, exist_ok=True, mode=0o755)
            Path(HIGH_CONFIDENCE_DIR).mkdir(parents=True, exist_ok=True, mode=0o755)
            logger.info(f"æ¨£æœ¬æ ¹ç›®éŒ„: {self.collection_root}")
            if self.save_uncertain_samples:
                logger.info(f"ä¸­ç­‰ä¿¡å¿ƒåº¦æ¨£æœ¬å„²å­˜ç›®éŒ„: {MEDIUM_CONFIDENCE_DIR}")
                logger.info(f"ä¿¡å¿ƒåº¦ç¯„åœ: {uncertain_conf_range[0]:.2f} - {uncertain_conf_range[1]:.2f}")
            if self.save_high_confidence:
                logger.info(f"é«˜ä¿¡å¿ƒåº¦æ¨£æœ¬å„²å­˜ç›®éŒ„: {HIGH_CONFIDENCE_DIR}")
                logger.info(f"é«˜ä¿¡å¿ƒåº¦é–¾å€¼: > {self.high_conf_threshold:.2f}")
            logger.info(f"æœ€å¤§å­˜å„²æ•¸é‡: {max_samples} å¼µ")
            logger.info(f"å„²å­˜æ™‚é–“é–“éš”: {save_interval} ç§’")
            logger.info(f"è‡ªå‹•æ¨™è¨»: {'å•Ÿç”¨' if save_annotations else 'åœç”¨'}")
            logger.info(f"å„²å­˜æ¨¡å¼: {'å®Œæ•´ç•«é¢' if save_full_frame else 'è£å‰ªå€åŸŸ'}")

        # å…‰ç…§åº¦ç›£æ§é…ç½®
        self.enable_illumination_monitoring = config.enable_illumination_monitoring
        self.illumination_warning_threshold = config.illumination_warning_threshold
        self.illumination_pause_threshold = config.illumination_pause_threshold
        self.illumination_check_interval = config.illumination_check_interval
        self.last_illumination_check = 0.0
        self.current_illumination = 0
        self.illumination_paused = False
        self.last_illumination_status = 'ok'  # ä¿å­˜ä¸Šæ¬¡çš„å®Œæ•´ç‹€æ…‹

        if self.enable_illumination_monitoring:
            logger.info(f"å…‰ç…§åº¦ç›£æ§å·²å•Ÿç”¨")
            logger.info(f"  - è­¦å‘Šé–¾å€¼: {self.illumination_warning_threshold}")
            logger.info(f"  - æš«åœ/æ¢å¾©é–¾å€¼: {self.illumination_pause_threshold}")

        # è‡ªå‹•é¸æ“‡æ¨¡å‹
        actual_model_path = self._auto_select_model(model_path, fallback_to_pretrained)

        if actual_model_path is None:
            raise FileNotFoundError("æ‰¾ä¸åˆ°ä»»ä½•å¯ç”¨çš„æ¨¡å‹æª”æ¡ˆ")

        # æ ¹æ“šå‰¯æª”åè¼‰å…¥å°æ‡‰çš„æ¨ç†å¼•æ“
        ext = Path(actual_model_path).suffix.lower()

        if ext == '.bin':
            if not HOBOT_DNN_AVAILABLE:
                raise RuntimeError(
                    f"æ‰¾åˆ° BIN æ¨¡å‹ä½† hobot_dnn åº«æœªå®‰è£\n"
                    f"è«‹åœ¨ RDK X5 ä¸Šå®‰è£: pip install hobot_dnn"
                )
            self._load_hobot_model(actual_model_path)
        elif ext == '.rknn':
            if not RKNN_AVAILABLE:
                raise RuntimeError(
                    f"æ‰¾åˆ° RKNN æ¨¡å‹ä½† rknnlite åº«æœªå®‰è£\n"
                    f"è«‹åœ¨ Orange Pi 5 ä¸Šå®‰è£:\n"
                    f"  sudo apt update\n"
                    f"  sudo apt install python3-rknnlite2"
                )
            self._load_rknn_model(actual_model_path)
        elif ext in ['.onnx', '.pt']:
            raise RuntimeError(f"åµæ¸¬ä¸æ”¯æ´ {ext} æ ¼å¼ï¼ˆåƒ…ç”¨æ–¼è¨“ç·´ï¼‰ã€‚è«‹ä½¿ç”¨ deploy_model.py è½‰æ›ç‚º .bin (RDK X5) æˆ– .rknn (Orange Pi 5)")
        else:
            raise RuntimeError(f"ä¸æ”¯æ´çš„æ¨¡å‹æ ¼å¼: {ext}ã€‚è«‹ä½¿ç”¨ .bin (RDK X5) æˆ– .rknn (Orange Pi 5)")

        logger.info(f"AIèšŠå­åµæ¸¬å™¨å·²åˆå§‹åŒ–ï¼Œä½¿ç”¨ {self.backend.upper()} å¾Œç«¯")

    def _auto_select_model(self, model_path: Optional[str], fallback: bool) -> Optional[str]:
        """
        è‡ªå‹•é¸æ“‡æœ€ä½³æ¨¡å‹ï¼ˆRKNN â†’ ONNX â†’ PyTorchï¼‰

        Args:
            model_path: ç”¨æˆ¶æŒ‡å®šçš„æ¨¡å‹è·¯å¾‘
            fallback: æ˜¯å¦å…è¨±å›é€€åˆ°é è¨“ç·´æ¨¡å‹

        Returns:
            é¸ä¸­çš„æ¨¡å‹å®Œæ•´è·¯å¾‘ï¼Œæˆ– None
        """
        # å¦‚æœç”¨æˆ¶æŒ‡å®šäº†å®Œæ•´è·¯å¾‘ä¸”å­˜åœ¨ï¼Œç›´æ¥ä½¿ç”¨
        if model_path and os.path.exists(model_path):
            logger.info(f"ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹: {model_path}")
            return model_path

        # å»ºç«‹æœå°‹è·¯å¾‘åˆ—è¡¨
        search_paths = []

        # ç²å–è…³æœ¬ç›®éŒ„å’Œå°ˆæ¡ˆæ ¹ç›®éŒ„
        script_dir = Path(__file__).resolve().parent
        project_root = script_dir.parent

        if model_path:
            # ç”¨æˆ¶æŒ‡å®šäº†åŸºæœ¬åç¨±ï¼ˆç„¡å‰¯æª”åï¼‰
            base_path = Path(model_path)
            base_name = base_path.stem if base_path.suffix else str(base_path)

            # å¦‚æœæŒ‡å®šäº†ç›®éŒ„ï¼Œä½¿ç”¨è©²ç›®éŒ„
            if base_path.parent.name:
                base_dir = base_path.parent
                search_paths.extend([
                    base_dir / f"{base_name}.bin",
                    base_dir / f"{base_name}.rknn"
                ])

        # æ”¯æ´å…©ç¨®ç›®éŒ„çµæ§‹ï¼ŒæŒ‰å„ªå…ˆé †åºæœå°‹
        models_dirs = [
            script_dir / 'models',        # python/models/ (ç•¶å‰ç›®éŒ„ä¸‹)
            project_root / 'models',      # ../models/ (å°ˆæ¡ˆæ ¹ç›®éŒ„)
        ]

        for models_dir in models_dirs:
            if models_dir.exists():
                for default_name in ['mosquito_yolov8', 'mosquito', 'yolov8n']:
                    search_paths.extend([
                        models_dir / f"{default_name}.bin",
                        models_dir / f"{default_name}.rknn"
                    ])

        # å˜—è©¦æ‰¾åˆ°ç¬¬ä¸€å€‹å­˜åœ¨çš„æ¨¡å‹
        for path in search_paths:
            if path.exists():
                logger.info(f"âœ“ æ‰¾åˆ°æ¨¡å‹: {path}")
                return str(path)

        # æœªæ‰¾åˆ°ç¡¬é«”åŠ é€Ÿæ¨¡å‹
        logger.error("æœªæ‰¾åˆ°ç¡¬é«”åŠ é€Ÿæ¨¡å‹ (.bin æˆ– .rknn)")
        logger.error(f"æœå°‹çš„ç›®éŒ„: {', '.join(str(d) for d in models_dirs if d.exists())}")
        logger.error(f"é æœŸçš„æª”å: mosquito_yolov8.{{bin,rknn}} æˆ– mosquito.{{bin,rknn}}")
        logger.error("è«‹ä½¿ç”¨ deploy_model.py å°‡è¨“ç·´å¥½çš„æ¨¡å‹è½‰æ›ç‚ºå°æ‡‰æ ¼å¼")
        return None

    def _load_hobot_model(self, model_path: str):
        """è¼‰å…¥ RDK X5 BIN æ¨¡å‹ï¼ˆBPU åŠ é€Ÿï¼‰"""
        logger.info(f"è¼‰å…¥ RDK X5 BIN æ¨¡å‹: {model_path}")

        # è¼‰å…¥æ¨¡å‹
        self.hobot_models = dnn.load(model_path)

        # ç²å–è¼¸å…¥è¼¸å‡ºä¿¡æ¯
        logger.info(f"BPU æ¨¡å‹å·²è¼‰å…¥ï¼Œè¼¸å…¥æ•¸é‡: {len(self.hobot_models[0].inputs)}")
        logger.info(f"BPU æ¨¡å‹è¼¸å‡ºæ•¸é‡: {len(self.hobot_models[0].outputs)}")

        self.backend = 'hobot_dnn'
        logger.info("âœ“ RDK X5 BPU åŠ é€Ÿå·²å•Ÿç”¨")

    def _load_rknn_model(self, model_path: str):
        """è¼‰å…¥ RKNN æ¨¡å‹ï¼ˆNPU åŠ é€Ÿï¼‰"""
        logger.info(f"è¼‰å…¥ RKNN æ¨¡å‹: {model_path}")
        self.rknn = RKNNLite()

        ret = self.rknn.load_rknn(model_path)
        if ret != 0:
            raise RuntimeError(f'è¼‰å…¥ RKNN æ¨¡å‹å¤±æ•—: {ret}')

        ret = self.rknn.init_runtime(core_mask=RKNNLite.NPU_CORE_0)
        if ret != 0:
            raise RuntimeError(f'åˆå§‹åŒ– RKNN åŸ·è¡Œç’°å¢ƒå¤±æ•—: {ret}')

        self.backend = 'rknn'
        logger.info("âœ“ RKNN NPU åŠ é€Ÿå·²å•Ÿç”¨")

    def _check_sample_count(self) -> bool:
        """
        æª¢æŸ¥å·²å„²å­˜æ¨£æœ¬æ•¸æ˜¯å¦é”åˆ°ä¸Šé™

        Returns:
            True: å¯ä»¥ç¹¼çºŒå„²å­˜
            False: å·²é”åˆ°æœ€å¤§æ•¸é‡é™åˆ¶ï¼Œæ‡‰æš«åœå„²å­˜
        """
        try:
            # è¨ˆç®—å·²å„²å­˜çš„æ¨£æœ¬æ•¸ï¼ˆsample_collection ä¸‹æ‰€æœ‰ .jpgï¼‰
            sample_count = len(list(self.collection_root.rglob("*.jpg")))

            if sample_count >= self.max_samples:
                logger.warning(f"âš  å·²å„²å­˜æ¨£æœ¬æ•¸å·²é”ä¸Šé™ ({sample_count}/{self.max_samples})ï¼Œæš«åœå„²å­˜")
                return False

            return True
        except Exception as e:
            logger.error(f"ç„¡æ³•æª¢æŸ¥æ¨£æœ¬æ•¸: {e}")
            return False

    def _is_frame_duplicate(self, frame: np.ndarray) -> bool:
        """
        æª¢æŸ¥ç•¶å‰ç•«é¢æ˜¯å¦èˆ‡ä¸Šæ¬¡å„²å­˜çš„ç•«é¢é‡è¤‡

        Args:
            frame: ç•¶å‰å½±åƒ

        Returns:
            True: ç•«é¢é‡è¤‡
            False: ç•«é¢ä¸é‡è¤‡
        """

        # æª¢æŸ¥æ™‚é–“é–“éš”ï¼Œè·é›¢ä¸Šæ¬¡å„²å­˜æ˜¯å¦è¶…é save_interval
        current_time = time.time()
        if current_time - self.last_save_time < self.save_interval:
            return True  # æ™‚é–“é–“éš”æœªé”åˆ°ï¼Œè¦–ç‚ºé‡è¤‡

        # è¨ˆç®—ç•¶å‰ç•«é¢çš„é›œæ¹Šå€¼
        frame_hash = hashlib.md5(frame.tobytes()).hexdigest()

        # èˆ‡ä¸Šæ¬¡å„²å­˜ç•«é¢æ¯”è¼ƒ
        if self.last_saved_hash == frame_hash:
            return True  # å…§å®¹é‡è¤‡

        self.last_saved_hash = frame_hash
        self.last_save_time = current_time
        return False

    def estimate_illumination(self, frame: np.ndarray) -> int:
        """
        ä¼°è¨ˆå½±åƒå…‰ç…§åº¦ï¼ˆäº®åº¦å€¼ 0-255ï¼‰

        Args:
            frame: è¼¸å…¥å½±åƒï¼ˆBGR æ ¼å¼ï¼‰

        Returns:
            ä¼°è¨ˆçš„äº®åº¦å€¼ï¼ˆ0-255 ç¯„åœï¼‰
        """
        # è½‰æ›ç‚ºç°åº¦åœ–
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # è¨ˆç®—å¹³å‡äº®åº¦å€¼
        brightness = int(np.mean(gray))

        # æ›´æ–°ç•¶å‰äº®åº¦å€¼
        self.current_illumination = brightness

        return brightness

    def check_illumination_status(self, frame: np.ndarray) -> Dict[str, any]:
        """
        æª¢æŸ¥å…‰ç…§åº¦ç‹€æ…‹ï¼Œä¸¦æ ¹æ“šé–¾å€¼æ±ºå®šæ˜¯å¦æš«åœ AI è¾¨è­˜

        Args:
            frame: è¼¸å…¥å½±åƒ

        Returns:
            åŒ…å«å…‰ç…§åº¦è³‡è¨Šå’Œç‹€æ…‹çš„å­—å…¸
        """
        if not self.enable_illumination_monitoring:
            return {
                'illumination': 0,
                'status': 'disabled',
                'paused': False,
                'message': ''
            }

        current_time = time.time()

        # æª¢æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°å…‰ç…§åº¦ï¼ˆé¿å…æ¯å¹€éƒ½è¨ˆç®—ï¼‰
        if current_time - self.last_illumination_check < self.illumination_check_interval:
            # ä½¿ç”¨ä¿å­˜çš„å®Œæ•´ç‹€æ…‹ï¼ˆåŒ…å« warningï¼‰
            return {
                'illumination': self.current_illumination,
                'status': self.last_illumination_status,
                'paused': self.illumination_paused,
                'message': ''
            }

        # ä¼°è¨ˆå…‰ç…§åº¦
        illumination = self.estimate_illumination(frame)
        self.last_illumination_check = current_time

        # åˆ¤æ–·å…‰ç…§åº¦ç‹€æ…‹
        status = 'ok'
        message = ''

        if illumination < self.illumination_pause_threshold:
            # å…‰ç·šå¤ªæš—ï¼Œæš«åœ AI è¾¨è­˜
            self.illumination_paused = True
            status = 'paused'
            message = f'Too Dark ({illumination}/255) AI Paused'
        elif illumination < self.illumination_warning_threshold:
            # å…‰ç…§åº¦ç¨ä½ï¼Œä½†æœªé”æš«åœé–¾å€¼
            self.illumination_paused = False
            status = 'warning'
            message = f'Low Light ({illumination}/255) May Affect Accuracy'
        else:
            # å…‰ç…§åº¦æ­£å¸¸
            self.illumination_paused = False
            status = 'ok'
            message = f'Light OK ({illumination}/255)'

        # ä¿å­˜ç•¶å‰ç‹€æ…‹ä¾›ä¸‹æ¬¡é–“éš”æœŸé–“ä½¿ç”¨
        self.last_illumination_status = status

        return {
            'illumination': illumination,
            'status': status,
            'paused': self.illumination_paused,
            'message': message
        }

    def _save_uncertain_sample(self, frame: np.ndarray, detection: Dict):
        """
        å„²å­˜æ¨£æœ¬åœ–ç‰‡ï¼ˆä¸­ç­‰æˆ–é«˜ä¿¡å¿ƒåº¦ï¼Œä¾è¨­å®šåˆ¤æ–·ï¼‰ä¸¦å¯é¸ç”Ÿæˆ YOLO æ ¼å¼æ¨™è¨»æ–‡ä»¶

        Args:
            frame: åŸå§‹å½±åƒ
            detection: æª¢æ¸¬çµæœï¼ˆå« bbox, confidence ç­‰è³‡è¨Šï¼‰
        """
        # ä¾è¨­å®šåˆ¤æ–·æ˜¯å¦éœ€è¦ä¿å­˜ï¼ˆä¸­ç­‰æˆ–é«˜ä¿¡å¿ƒåº¦ï¼‰
        confidence = detection.get('confidence', 0.0)

        # éæ¿¾ç•°å¸¸ä¿¡å¿ƒåº¦å€¼ï¼ˆæ’é™¤ confidence == 0.999ï¼‰
        if confidence >= 0.999:
            logger.debug(f"å·²è·³éä¿¡å¿ƒåº¦>=0.999çš„ç•°å¸¸æª¢æ¸¬æ¨£æœ¬")
            return

        is_medium = self.save_uncertain_samples and (self.uncertain_conf_range[0] <= confidence <= self.uncertain_conf_range[1])
        is_high = self.save_high_confidence and (confidence > self.high_conf_threshold)

        if not (is_medium or is_high):
            return
        # æª¢æŸ¥æ¨£æœ¬æ•¸æ˜¯å¦é”åˆ°ä¸Šé™
        if not self._check_sample_count():
            return

        # æª¢æŸ¥æ™‚é–“é–“éš”å’Œå…§å®¹é‡è¤‡
        if self._is_frame_duplicate(frame):
            return

        try:
            x, y, w, h = detection['bbox']
            class_id = detection.get('class_id', 0)

            # ç”ŸæˆåŸºç¤æª”åï¼šæ™‚é–“æˆ³_ä¿¡å¿ƒåº¦
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S_%f")
            base_filename = f"mosquito_{timestamp}_conf{confidence:.3f}"

            # æ±ºå®šå„²å­˜çš„åœ–ç‰‡å…§å®¹
            if self.save_full_frame:
                # å„²å­˜å®Œæ•´ç•«é¢
                image_to_save = frame.copy()
                # åœ¨å®Œæ•´ç•«é¢ä¸Šç¹ªè£½é‚Šç•Œæ¡†
                cv2.rectangle(image_to_save, (x, y), (x + w, y + h), (0, 255, 0), 2)
                cv2.putText(image_to_save, f"{confidence:.2f}", (x, y - 10),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
            else:
                # åƒ…è£å‰ªæª¢æ¸¬å€åŸŸï¼ˆå¸¶å®‰å…¨é‚Šç•Œï¼‰
                margin = 10
                img_h, img_w = frame.shape[:2]
                y1 = max(0, y - margin)
                y2 = min(img_h, y + h + margin)
                x1 = max(0, x - margin)
                x2 = min(img_w, x + w + margin)
                image_to_save = frame[y1:y2, x1:x2].copy()

            # ç›®éŒ„ï¼šå›ºå®šè‡³ config å®šç¾©çš„åˆ†é¡ç›®éŒ„
            dest_dir = Path(MEDIUM_CONFIDENCE_DIR) if is_medium else Path(HIGH_CONFIDENCE_DIR)
            dest_dir.mkdir(parents=True, exist_ok=True, mode=0o755)

            # å„²å­˜åœ–ç‰‡
            image_path = dest_dir / f"{base_filename}.jpg"
            cv2.imwrite(str(image_path), image_to_save)
            # è¨­å®šæª”æ¡ˆæ¬Šé™ 644 (rw-r--r--)
            try:
                os.chmod(str(image_path), 0o644)
            except Exception:
                pass  # Windows ä¸æ”¯æ´ï¼Œå¿½ç•¥

            # ç”Ÿæˆä¸¦å„²å­˜ YOLO æ ¼å¼æ¨™è¨»æ–‡ä»¶
            if self.save_annotations:
                annotation_path = dest_dir / f"{base_filename}.txt"
                self._save_yolo_annotation(annotation_path, frame.shape, detection)
                # è¨­å®šæ¨™è¨»æª”æ¬Šé™ 644
                try:
                    os.chmod(str(annotation_path), 0o644)
                except Exception:
                    pass  # Windows ä¸æ”¯æ´ï¼Œå¿½ç•¥

            self.save_counter += 1

            if self.save_counter % 10 == 0:
                logger.info(f"å·²å„²å­˜ {self.save_counter} å¼µæ¨£æœ¬{'ï¼ˆå«æ¨™è¨»ï¼‰' if self.save_annotations else ''}")

        except Exception as e:
            logger.error(f"å„²å­˜æ¨£æœ¬å¤±æ•—: {e}")

    def _save_yolo_annotation(self, annotation_path: Path, frame_shape: Tuple[int, int, int], detection: Dict):
        """
        ç”Ÿæˆä¸¦å„²å­˜ YOLO æ ¼å¼æ¨™è¨»æ–‡ä»¶

        YOLO æ ¼å¼ï¼šclass_id x_center y_center width heightï¼ˆæ­¸ä¸€åŒ–åº§æ¨™ 0-1ï¼‰

        Args:
            annotation_path: æ¨™è¨»æ–‡ä»¶è·¯å¾‘
            frame_shape: åŸå§‹å½±åƒå°ºå¯¸ (height, width, channels)
            detection: æª¢æ¸¬çµæœ
        """
        try:
            img_h, img_w = frame_shape[:2]
            x, y, w, h = detection['bbox']
            class_id = detection.get('class_id', 0)

            # è¨ˆç®—ä¸­å¿ƒé»å’Œå°ºå¯¸ï¼ˆæ­¸ä¸€åŒ–åˆ° 0-1ï¼‰
            x_center = (x + w / 2) / img_w
            y_center = (y + h / 2) / img_h
            width_norm = w / img_w
            height_norm = h / img_h

            # ç¢ºä¿åº§æ¨™åœ¨æœ‰æ•ˆç¯„åœå…§
            x_center = max(0.0, min(1.0, x_center))
            y_center = max(0.0, min(1.0, y_center))
            width_norm = max(0.0, min(1.0, width_norm))
            height_norm = max(0.0, min(1.0, height_norm))

            # å¯«å…¥ YOLO æ ¼å¼æ¨™è¨»ï¼ˆä¸€è¡Œä¸€å€‹ç›®æ¨™ï¼‰
            with open(annotation_path, 'w') as f:
                f.write(f"{class_id} {x_center:.6f} {y_center:.6f} {width_norm:.6f} {height_norm:.6f}\n")

        except Exception as e:
            logger.error(f"ç”Ÿæˆæ¨™è¨»æ–‡ä»¶å¤±æ•—: {e}")


    def detect(self, frame: np.ndarray, is_dual_left: bool = False) -> Tuple[List[Dict], np.ndarray, Dict]:
        """
        ä½¿ç”¨ AI æ¨¡å‹åµæ¸¬èšŠå­ï¼ˆè‡ªå‹•é¸æ“‡ RKNN/ONNX/PyTorchï¼‰

        Args:
            frame: è¼¸å…¥å½±åƒï¼ˆBGRæ ¼å¼ï¼‰
            is_dual_left: æ˜¯å¦ç‚ºé›™ç›®å·¦çœ¼ç•«é¢ï¼ˆåªéæ¿¾ä¸Šä¸‹é‚Šç·£ï¼‰

        Returns:
            (åµæ¸¬çµæœåˆ—è¡¨ï¼ŒåŒ…å«bboxå’Œconfidenceï¼Œè™•ç†å¾Œçš„å½±åƒï¼Œå…‰ç…§åº¦è³‡è¨Š)
        """
        try:
            # æª¢æŸ¥å…‰ç…§åº¦ç‹€æ…‹
            illumination_info = self.check_illumination_status(frame)

            # å¦‚æœå…‰ç…§åº¦éä½ï¼Œæš«åœ AI è¾¨è­˜
            if illumination_info['paused']:
                return [], frame, illumination_info

            if self.detection_mode == 'tiling':
                detections, result_frame = self._detect_tiled(frame)
            else:
                if self.backend == 'hobot_dnn':
                    detections, result_frame = self._detect_hobot(frame)
                elif self.backend == 'rknn':
                    detections, result_frame = self._detect_rknn(frame)
                else:
                    raise RuntimeError(f"æœªçŸ¥çš„æ¨ç†å¾Œç«¯: {self.backend}")

            # éæ¿¾é‚Šç•Œå€åŸŸçš„æª¢æ¸¬çµæœ
            if self.detection_margin > 0 and detections:
                detections = self._filter_margin_detections(detections, frame.shape[:2], is_dual_left)

            # å„²å­˜æ¨£æœ¬ï¼ˆä¸­/é«˜ä¿¡å¿ƒåº¦ç”± _save_uncertain_sample åˆ¤æ–·ï¼‰
            if (self.save_uncertain_samples or self.save_high_confidence) and detections:
                for detection in detections:
                    self._save_uncertain_sample(frame, detection)

            return detections, result_frame, illumination_info

        except KeyboardInterrupt:
            # è®“ KeyboardInterrupt æ­£å¸¸å‚³æ’­ï¼Œè§¸ç™¼å„ªé›…é—œé–‰
            raise
        except RuntimeError as e:
            logger.error(f"AI æ¨ç†å¤±æ•— (Runtime): {e}")
            illumination_info = self.check_illumination_status(frame)
            return [], frame, illumination_info
        except MemoryError as e:
            logger.error(f"è¨˜æ†¶é«”ä¸è¶³ç„¡æ³•åŸ·è¡Œæ¨ç†: {e}")
            illumination_info = self.check_illumination_status(frame)
            return [], frame, illumination_info
        except Exception as e:
            logger.error(f"AI åµæ¸¬ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}")
            illumination_info = self.check_illumination_status(frame)
            return [], frame, illumination_info

    def _filter_margin_detections(self, detections: List[Dict], frame_shape: Tuple[int, int], is_dual_left: bool = False) -> List[Dict]:
        """
        éæ¿¾æ‰ä½æ–¼ç•«é¢é‚Šç·£å€åŸŸçš„æª¢æ¸¬çµæœ

        Args:
            detections: æª¢æ¸¬çµæœåˆ—è¡¨
            frame_shape: å½±åƒå°ºå¯¸ (height, width)
            is_dual_left: æ˜¯å¦ç‚ºé›™ç›®å·¦çœ¼ï¼ˆTrue æ™‚åªéæ¿¾ä¸Šä¸‹é‚Šç·£ï¼‰

        Returns:
            éæ¿¾å¾Œçš„æª¢æ¸¬çµæœåˆ—è¡¨
        """
        if self.detection_margin <= 0:
            return detections

        h, w = frame_shape
        margin_y = int(h * self.detection_margin)

        filtered = []
        for det in detections:
            cx, cy = det['center']

            # é›™ç›®å·¦çœ¼ï¼šåªæª¢æŸ¥ä¸Šä¸‹é‚Šç•Œï¼ˆå·¦å³é‚Šç•Œä¸æ˜¯çœŸæ­£çš„é‚Šç·£ï¼‰
            if is_dual_left:
                if margin_y <= cy < h - margin_y:
                    filtered.append(det)
            else:
                # å–®ç›®ï¼šæª¢æŸ¥å››é‚Š
                margin_x = int(w * self.detection_margin)
                if (margin_x <= cx < w - margin_x) and (margin_y <= cy < h - margin_y):
                    filtered.append(det)

        return filtered

    def _run_backend_once(self, img: np.ndarray) -> List[Dict]:
        """
        ä½¿ç”¨ç›®å‰å¾Œç«¯åœ¨å–®å¼µå½±åƒä¸Šæ¨ç†ï¼ˆå›å‚³åµæ¸¬çµæœï¼Œåº§æ¨™ç›¸å°æ–¼ imgï¼‰ã€‚
        """
        if self.backend == 'hobot_dnn':
            dets, _ = self._detect_hobot(img)
            return dets
        elif self.backend == 'rknn':
            dets, _ = self._detect_rknn(img)
            return dets
        else:
            raise RuntimeError(f"æœªçŸ¥çš„æ¨ç†å¾Œç«¯: {self.backend}")

    def _detect_tiled(self, frame: np.ndarray) -> Tuple[List[Dict], np.ndarray]:
        """
        å¹³é‹ª(tiling)æ¨ç†ï¼š
        - ä»¥ imgsz ç‚ºæ–¹å½¢è¦–çª—å°åŸåœ–æ»‘å‹•ï¼Œè¦–çª—é–“æœ‰ä¸€å®šé‡ç–Š
        - å„è¦–çª—å…§ç¨ç«‹æ¨ç†ï¼Œè½‰æ›å›å…¨åŸŸåº§æ¨™
        - ä»¥å…¨åŸŸ NMS åˆä½µé‡ç–Šæ¡†ï¼Œé¿å…é‡è¤‡è¨ˆæ•¸
        """
        h, w = frame.shape[:2]
        tile = int(self.imgsz)
        # é‡ç–Šæ¯”ä¾‹è½‰ç‚ºæ­¥é•·ï¼ˆåƒç´ ï¼‰
        stride = max(1, int(tile * (1.0 - self.tile_overlap)))

        merged: List[Dict] = []

        # ç”¢ç”Ÿæ»‘å‹•è¦–çª—çš„èµ·é»ï¼ˆç¢ºä¿è¦†è“‹åˆ°é‚Šç•Œï¼‰
        xs = list(range(0, max(1, w - tile + 1), stride))
        ys = list(range(0, max(1, h - tile + 1), stride))
        if len(xs) == 0:
            xs = [0]
        if len(ys) == 0:
            ys = [0]
        if xs[-1] != max(0, w - tile):
            xs.append(max(0, w - tile))
        if ys[-1] != max(0, h - tile):
            ys.append(max(0, h - tile))

        for y0 in ys:
            for x0 in xs:
                x1 = min(w, x0 + tile)
                y1 = min(h, y0 + tile)
                patch = frame[y0:y1, x0:x1]

                # åœ¨å­å½±åƒä¸Šæ¨ç†ï¼ˆåº§æ¨™ç›¸å°æ–¼ patchï¼‰
                dets = self._run_backend_once(patch)

                # è½‰ç‚ºå…¨åŸŸåº§æ¨™ä¸¦æš«å­˜
                for d in dets:
                    bx, by, bw, bh = d['bbox']
                    cx, cy = d['center']
                    nd = d.copy()
                    nd['bbox'] = (bx + x0, by + y0, bw, bh)
                    nd['center'] = (cx + x0, cy + y0)
                    merged.append(nd)

        # å…¨åŸŸ NMS åˆä½µ
        merged = self._nms(merged, self.iou_threshold)
        return merged, frame

    def _nms(self, detections: List[Dict], iou_thresh: float) -> List[Dict]:
        """ç°¡å–®çš„å…¨åŸŸ NMSï¼ˆæŒ‰ä¿¡å¿ƒåº¦æ’åºï¼Œç§»é™¤ IoU éé«˜çš„é‡ç–Šæ¡†ï¼‰"""
        if not detections:
            return []

        # è½‰ç‚º (x1,y1,x2,y2,conf,idx)
        boxes = []
        for i, d in enumerate(detections):
            x, y, w, h = d['bbox']
            boxes.append((x, y, x + w, y + h, d['confidence'], i))

        # ä¾ä¿¡å¿ƒåº¦ç”±é«˜åˆ°ä½æ’åº
        boxes.sort(key=lambda b: b[4], reverse=True)

        keep = []
        suppressed = set()
        for i in range(len(boxes)):
            if i in suppressed:
                continue
            keep.append(detections[boxes[i][5]])
            xi1, yi1, xi2, yi2, ci, _ = boxes[i]
            for j in range(i + 1, len(boxes)):
                if j in suppressed:
                    continue
                xj1, yj1, xj2, yj2, cj, _ = boxes[j]
                if self._iou((xi1, yi1, xi2, yi2), (xj1, yj1, xj2, yj2)) > iou_thresh:
                    suppressed.add(j)

        return keep

    @staticmethod
    def _iou(a: Tuple[int, int, int, int], b: Tuple[int, int, int, int]) -> float:
        """è¨ˆç®—å…©å€‹æ¡†çš„ IoUï¼Œæ¡†æ ¼å¼ç‚º (x1,y1,x2,y2)"""
        ax1, ay1, ax2, ay2 = a
        bx1, by1, bx2, by2 = b
        inter_x1 = max(ax1, bx1)
        inter_y1 = max(ay1, by1)
        inter_x2 = min(ax2, bx2)
        inter_y2 = min(ay2, by2)
        inter_w = max(0, inter_x2 - inter_x1)
        inter_h = max(0, inter_y2 - inter_y1)
        inter = inter_w * inter_h
        area_a = max(0, ax2 - ax1) * max(0, ay2 - ay1)
        area_b = max(0, bx2 - bx1) * max(0, by2 - by1)
        union = area_a + area_b - inter
        if union <= 0:
            return 0.0
        return inter / union

    def _detect_hobot(self, frame: np.ndarray) -> Tuple[List[Dict], np.ndarray]:
        """ä½¿ç”¨ RDK X5 BPU (hobot_dnn) æ¨ç†"""
        # é è™•ç†ï¼šèª¿æ•´å¤§å°ä¸¦è½‰æ›ç‚º NV12 æ ¼å¼ï¼ˆRDK X5 BPU å°ˆç”¨ï¼‰
        img = cv2.resize(frame, (self.imgsz, self.imgsz))

        # å°‡ BGR è½‰ç‚º NV12 (BPU åŸç”Ÿæ ¼å¼)
        nv12_data = dnn.pyimg_to_nv12(img)

        # BPU æ¨ç†
        outputs = self.hobot_models[0].forward(nv12_data)

        # å–å¾—è¼¸å‡ºæ•¸æ“šï¼ˆå‡è¨­ç¬¬ä¸€å€‹è¼¸å‡ºæ˜¯æª¢æ¸¬çµæœï¼‰
        output_data = outputs[0].buffer

        # å¾Œè™•ç†ï¼ˆYOLO æ ¼å¼ï¼‰
        detections = self._parse_yolo_output(output_data, frame.shape[:2])

        return detections, frame

    def _detect_rknn(self, frame: np.ndarray) -> Tuple[List[Dict], np.ndarray]:
        """ä½¿ç”¨ RKNN NPU æ¨ç†"""
        # é è™•ç†
        img = cv2.resize(frame, (self.imgsz, self.imgsz))
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        # è³‡è¨Šæ—¥èªŒï¼šè¨˜éŒ„é è™•ç†å¾Œçš„å½±åƒçµ±è¨ˆ
        logger.debug(f"ğŸ“Š é è™•ç†å¾Œå½±åƒçµ±è¨ˆ - min: {img.min()}, max: {img.max()}, mean: {img.mean():.2f}")

        # æ·»åŠ  batch ç¶­åº¦ï¼š(H, W, C) -> (1, H, W, C)
        img = np.expand_dims(img, axis=0)

        # NPU æ¨ç†
        try:
            outputs = self.rknn.inference(inputs=[img])
        except KeyboardInterrupt:
            # è®“ KeyboardInterrupt æ­£å¸¸å‚³æ’­ï¼Œè§¸ç™¼å„ªé›…é—œé–‰
            raise
        except Exception as e:
            logger.error(f"âŒ RKNN æ¨ç†ç•°å¸¸: {type(e).__name__} - {e}")
            return [], frame

        # é©—è­‰è¼¸å‡ºå®Œæ•´æ€§
        if outputs is None:
            logger.warning("âš ï¸  RKNN æ¨ç†è¿”å› None")
            return [], frame

        if len(outputs) == 0:
            logger.warning("âš ï¸  RKNN æ¨ç†è¿”å›ç©ºåˆ—è¡¨")
            return [], frame

        # æª¢æŸ¥ç¬¬ä¸€å€‹è¼¸å‡ºå¼µé‡
        try:
            first_output = outputs[0]
            if first_output is None:
                logger.warning("âš ï¸  RKNN ç¬¬ä¸€å€‹è¼¸å‡ºç‚º None")
                return [], frame

            if hasattr(first_output, 'shape'):
                logger.debug(f"ğŸ“¦ RKNN è¼¸å‡ºå½¢ç‹€: {first_output.shape}, dtype: {first_output.dtype}")

                if len(first_output.shape) == 0 or first_output.size == 0:
                    logger.warning("âš ï¸  RKNN æ¨ç†è¼¸å‡ºç‚ºç©ºå¼µé‡")
                    return [], frame
            else:
                logger.warning(f"âš ï¸  RKNN è¼¸å‡ºä¸æ˜¯ ndarray: {type(first_output)}")
                return [], frame

        except KeyboardInterrupt:
            # è®“ KeyboardInterrupt æ­£å¸¸å‚³æ’­
            raise
        except Exception as e:
            logger.warning(f"âš ï¸  æª¢æŸ¥ RKNN è¼¸å‡ºå¤±æ•—: {e}")
            return [], frame

        # å¾Œè™•ç†ï¼ˆå‡è¨­ YOLO è¼¸å‡ºæ ¼å¼ï¼‰
        try:
            detections = self._parse_yolo_output(outputs[0], frame.shape[:2])
            logger.debug(f"âœ“ æ¨ç†æˆåŠŸ - æª¢æ¸¬åˆ° {len(detections)} å€‹ç›®æ¨™")
        except Exception as e:
            logger.error(f"âŒ å¾Œè™•ç†å¤±æ•—: {e}")
            return [], frame

        return detections, frame

    def _parse_yolo_output(self, output: np.ndarray, original_shape: Tuple[int, int]) -> List[Dict]:
        """
        è§£æ YOLO è¼¸å‡ºï¼ˆONNX/RKNN é€šç”¨ï¼‰

        âš ï¸ æ³¨æ„ï¼šè‡ªå‹•è™•ç†æœªæ­¸ä¸€åŒ–çš„è¼¸å‡ºï¼ˆæ‡‰ç”¨ sigmoidï¼‰

        Args:
            output: æ¨¡å‹è¼¸å‡ºå¼µé‡
            original_shape: åŸå§‹å½±åƒå°ºå¯¸ (height, width)

        Returns:
            åµæ¸¬çµæœåˆ—è¡¨
        """
        detections = []
        h_orig, w_orig = original_shape

        # YOLO è¼¸å‡ºæ ¼å¼: [batch, num_boxes, 85] æˆ– [batch, 85, num_boxes]
        # 85 = x_center, y_center, width, height, objectness, 80 classes

        # èª¿æ•´è¼¸å‡ºå½¢ç‹€
        if len(output.shape) == 3:
            if output.shape[2] == 85:
                pass  # [1, num_boxes, 85]
            elif output.shape[1] == 85:
                output = output.transpose(0, 2, 1)  # [1, 85, num_boxes] â†’ [1, num_boxes, 85]

        output = output[0]  # ç§»é™¤ batch ç¶­åº¦

        # æª¢æŸ¥æ˜¯å¦éœ€è¦ sigmoidï¼ˆèª¿è©¦ç”¨ï¼‰
        if len(output) > 0:
            max_objectness = np.max(output[:, 4])
            if max_objectness > 10.0:  # æ˜é¡¯æœªæ­¸ä¸€åŒ–
                logger.debug(f"æª¢æ¸¬åˆ°æœªæ­¸ä¸€åŒ–è¼¸å‡º (max objectness: {max_objectness:.2f})ï¼Œå°‡æ‡‰ç”¨ sigmoid")

        for detection in output:
            objectness = detection[4]

            # âš ï¸ é‡è¦ï¼šç¢ºä¿ objectness åœ¨ [0, 1] ç¯„åœå…§
            # æœ‰äº›æ¨¡å‹è¼¸å‡ºæœªç¶“ sigmoidï¼Œéœ€è¦æ‰‹å‹•è™•ç†
            if objectness > 1.0:
                objectness = 1.0 / (1.0 + np.exp(-objectness))  # sigmoid

            if objectness < self.confidence_threshold:
                continue

            # è§£æåº§æ¨™
            x_center, y_center, width, height = detection[:4]

            # è½‰æ›ç‚ºåŸå§‹å½±åƒåº§æ¨™
            x_center = x_center / self.imgsz * w_orig
            y_center = y_center / self.imgsz * h_orig
            width = width / self.imgsz * w_orig
            height = height / self.imgsz * h_orig

            x1 = int(x_center - width / 2)
            y1 = int(y_center - height / 2)
            w = int(width)
            h = int(height)

            # é¡åˆ¥æ¦‚ç‡ï¼ˆåŒæ¨£éœ€è¦ç¢ºä¿åœ¨ [0, 1] ç¯„åœï¼‰
            class_scores = detection[5:]
            # å°é¡åˆ¥åˆ†æ•¸æ‡‰ç”¨ sigmoidï¼ˆå¦‚æœéœ€è¦ï¼‰
            if np.max(class_scores) > 1.0:
                class_scores = 1.0 / (1.0 + np.exp(-class_scores))

            class_id = int(np.argmax(class_scores))
            confidence = float(objectness * class_scores[class_id])

            # æœ€çµ‚ç¢ºä¿ confidence åœ¨åˆç†ç¯„åœå…§
            confidence = min(1.0, max(0.0, confidence))

            if confidence >= self.confidence_threshold:
                detections.append({
                    'bbox': (x1, y1, w, h),
                    'confidence': confidence,
                    'class_id': class_id,
                    'class_name': f'class_{class_id}',
                    'center': (int(x_center), int(y_center))
                })

        return detections


    def get_largest_detection(self, detections: List[Dict]) -> Optional[Dict]:
        """
        ç²å–ä¿¡å¿ƒåº¦æœ€é«˜çš„åµæ¸¬çµæœ

        Args:
            detections: åµæ¸¬çµæœåˆ—è¡¨

        Returns:
            ä¿¡å¿ƒåº¦æœ€é«˜çš„åµæ¸¬çµæœï¼Œè‹¥ç„¡å‰‡è¿”å› None
        """
        if not detections:
            return None

        return max(detections, key=lambda det: det['confidence'])

    def get_center(self, detection: Dict) -> Tuple[int, int]:
        """
        ç²å–æª¢æ¸¬çµæœçš„ä¸­å¿ƒé»

        Args:
            detection: æª¢æ¸¬çµæœå­—å…¸

        Returns:
            ä¸­å¿ƒé»åº§æ¨™ (cx, cy)
        """
        return detection['center']

    def draw_detections(self, frame: np.ndarray, detections: List[Dict],
                       color: Tuple[int, int, int] = (0, 255, 0),
                       thickness: int = 2) -> np.ndarray:
        """
        åœ¨å½±åƒä¸Šç¹ªè£½AIåµæ¸¬æ¡†

        Args:
            frame: è¼¸å…¥å½±åƒ
            detections: åµæ¸¬çµæœåˆ—è¡¨
            color: é‚Šç•Œæ¡†é¡è‰² (B, G, R)
            thickness: ç·šæ¢ç²—ç´°

        Returns:
            ç¹ªè£½å¾Œçš„å½±åƒ
        """
        result = frame.copy()

        for detection in detections:
            x, y, w, h = detection['bbox']
            confidence = detection['confidence']
            class_name = detection['class_name']
            cx, cy = detection['center']

            # æ ¹æ“šä¿¡å¿ƒåº¦é¸æ“‡é¡è‰²ï¼ˆåŸºæ–¼ DEFAULT_UNCERTAIN_CONF_RANGEï¼‰
            conf_min, conf_max = self.uncertain_conf_range
            if confidence > conf_max:
                box_color = (0, 255, 0)  # ç¶ è‰²ï¼šé«˜ä¿¡å¿ƒåº¦ï¼ˆ> conf_maxï¼‰
            elif confidence >= conf_min:
                box_color = (0, 255, 255)  # é»ƒè‰²ï¼šä¸­ç­‰ä¿¡å¿ƒåº¦ï¼ˆconf_min åˆ° conf_maxï¼‰
            else:
                box_color = (0, 165, 255)  # æ©™è‰²ï¼šä½ä¿¡å¿ƒåº¦ï¼ˆ< conf_minï¼‰

            # ç¹ªè£½é‚Šç•Œæ¡†
            cv2.rectangle(result, (x, y), (x + w, y + h), box_color, thickness)

            # ç¹ªè£½ä¸­å¿ƒé»
            cv2.circle(result, (cx, cy), 3, (0, 0, 255), -1)

            # æ¨™è¨»é¡åˆ¥å’Œä¿¡å¿ƒåº¦
            label = f"{class_name}: {confidence:.2f}"
            (label_w, label_h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)
            cv2.rectangle(result, (x, y - label_h - 10), (x + label_w, y), box_color, -1)
            cv2.putText(result, label, (x, y - 5),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)

        return result

    def reset(self):
        """é‡ç½®åµæ¸¬å™¨ç‹€æ…‹ï¼ˆAIæ¨¡å‹ä¸éœ€è¦é‡ç½®ï¼‰"""
        logger.info("AIåµæ¸¬å™¨ç„¡éœ€é‡ç½®")

    def cleanup(self):
        """å„ªé›…é—œé–‰åµæ¸¬å™¨ï¼Œé‡‹æ”¾ç¡¬é«”åŠ é€Ÿè³‡æº"""
        logger.info("æ­£åœ¨æ¸…ç†åµæ¸¬å™¨è³‡æº...")
        try:
            if self.backend == 'rknn' and hasattr(self, 'rknn'):
                logger.info("æ­£åœ¨é‡‹æ”¾ RKNN æ¨¡å‹...")
                if hasattr(self.rknn, 'release'):
                    self.rknn.release()
                logger.info("âœ“ RKNN è³‡æºå·²é‡‹æ”¾")
        except Exception as e:
            logger.error(f"RKNN æ¸…ç†å¤±æ•—: {e}")

        try:
            if self.backend == 'hobot_dnn' and hasattr(self, 'hobot_models'):
                logger.info("æ­£åœ¨é‡‹æ”¾ BPU æ¨¡å‹...")
                # hobot_dnn çš„æ¨¡å‹åœ¨åƒåœ¾å›æ”¶æ™‚è‡ªå‹•é‡‹æ”¾
                logger.info("âœ“ BPU è³‡æºå·²é‡‹æ”¾")
        except Exception as e:
            logger.error(f"BPU æ¸…ç†å¤±æ•—: {e}")

        try:
            if self.backend == 'onnx' and hasattr(self, 'model'):
                logger.info("æ­£åœ¨é‡‹æ”¾ ONNX æ¨¡å‹...")
                if hasattr(self.model, 'close'):
                    self.model.close()
                logger.info("âœ“ ONNX è³‡æºå·²é‡‹æ”¾")
        except Exception as e:
            logger.error(f"ONNX æ¸…ç†å¤±æ•—: {e}")

        logger.info("âœ“ åµæ¸¬å™¨å·²æ¸…ç†å®Œæˆ")


def test_mosquito_detector():
    """æ¸¬è©¦AIèšŠå­åµæ¸¬å™¨ï¼ˆç¡¬é«”åŠ é€Ÿå°ˆç”¨ï¼‰"""
    logger.info("=== æ¸¬è©¦AIèšŠå­åµæ¸¬å™¨ ===")
    logger.info("ç¡¬é«”åŠ é€Ÿæ¨ç†ï¼šhobot_dnn (BPU) / rknnlite (NPU)")
    logger.info("æŒ‰ 'q' é€€å‡º, 's' å„²å­˜ç•¶å‰å¹€")
    logger.info("\næ³¨æ„ï¼šè«‹ç¢ºä¿å·²ä½¿ç”¨ deploy_model.py è½‰æ›æ¨¡å‹ç‚º .bin æˆ– .rknn æ ¼å¼")

    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        logger.error("ç„¡æ³•é–‹å•Ÿæ”åƒé ­")
        return

    try:
        # åˆå§‹åŒ–AIåµæ¸¬å™¨ï¼ˆç¡¬é«”åŠ é€Ÿå°ˆç”¨ï¼‰
        # æŒ‡å®šåŸºæœ¬åç¨±ï¼Œæœƒè‡ªå‹•æœå°‹ models/mosquito.{bin,rknn}
        detector = MosquitoDetector(
            model_path='models/mosquito',  # è‡ªå‹•é¸æ“‡ç¡¬é«”åŠ é€Ÿæ ¼å¼
            confidence_threshold=0.3,
            imgsz=config.imgsz  # å¾ mosquito.ini é…ç½®æ–‡ä»¶è®€å–ï¼Œå¯çµ±ä¸€ä¿®æ”¹
        )

        frame_count = 0
        fps_start = time.time()
        fps_counter = 0

        while True:
            ret, frame = cap.read()
            if not ret:
                break

            frame_count += 1
            fps_counter += 1

            # åŸ·è¡ŒAIåµæ¸¬
            detections, _ = detector.detect(frame)

            # ç¹ªè£½åµæ¸¬çµæœ
            result = detector.draw_detections(frame, detections)

            # è¨ˆç®— FPS
            if fps_counter >= 30:
                fps_elapsed = time.time() - fps_start
                fps = fps_counter / fps_elapsed
                fps_start = time.time()
                fps_counter = 0
            else:
                fps = 0

            # é¡¯ç¤ºåµæ¸¬æ•¸é‡å’ŒFPSï¼ˆåµæ¸¬æ•¸/ä½ä¿¡å¿ƒåµæ¸¬æ•¸ï¼‰
            cv2.putText(result, f"Detections: {len(detections)}/{detector.save_counter} | FPS: {fps:.1f}",
                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            mode_text = 'TILING' if detector.detection_mode == 'tiling' else 'WHOLE'
            cv2.putText(result, f"Backend: {detector.backend.upper()} | ImgSz: {detector.imgsz} | Mode: {mode_text}",
                       (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

            # å¦‚æœæœ‰åµæ¸¬åˆ°ï¼Œé«˜äº®æœ€ä½³çµæœ
            best = detector.get_largest_detection(detections)
            if best:
                x, y, w, h = best['bbox']
                cv2.rectangle(result, (x, y), (x + w, y + h), (0, 0, 255), 3)
                cx, cy = best['center']
                cv2.putText(result, f"Best: ({cx}, {cy})", (cx - 50, cy - 10),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)

                # é¡¯ç¤ºè©³ç´°è³‡è¨Š
                info = f"Class: {best['class_name']} | Conf: {best['confidence']:.2f}"
                cv2.putText(result, info, (10, 90),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)

            # é¡¯ç¤ºå½±åƒ
            cv2.imshow('AI Mosquito Detection', result)

            # éµç›¤æ§åˆ¶
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                break
            elif key == ord('s'):
                filename = f"detection_frame_{frame_count}.jpg"
                cv2.imwrite(filename, result)
                logger.info(f"å·²å„²å­˜: {filename}")

    except Exception as e:
        logger.error(f"éŒ¯èª¤: {e}")
        traceback.print_exc()
    finally:
        cap.release()
        cv2.destroyAllWindows()
        logger.info("æ¸¬è©¦å®Œæˆ")


if __name__ == "__main__":
    test_mosquito_detector()
