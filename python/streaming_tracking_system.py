"""
å®Œæ•´èšŠå­è¿½è¹¤ç³»çµ± + æ‰‹æ©Ÿä¸²æµ
æ•´åˆ AI æª¢æ¸¬ã€é›²å°è¿½è¹¤ã€å½±åƒä¸²æµæ–¼ä¸€é«”

âš ï¸ é€™æ˜¯å”¯ä¸€éœ€è¦é‹è¡Œçš„ç¨‹å¼ï¼
- âœ… åŒ…å« AI æª¢æ¸¬ (MosquitoDetector)
- âœ… åŒ…å«é›²å°æ§åˆ¶ (PT2DController)
- âœ… åŒ…å«è¿½è¹¤é‚è¼¯ (MosquitoTracker)
- âœ… åŒ…å«å½±åƒä¸²æµ (StreamingServer)
- âœ… AI è² è¼‰ä¸æœƒé‡è¤‡ï¼ˆæ¯å¹€åªæª¢æ¸¬ä¸€æ¬¡ï¼‰
- âœ… æ”¯æ´é›™ç›®æ”åƒé ­

ä½¿ç”¨æ–¹å¼ï¼š
    python streaming_tracking_system.py
"""

# Copyright 2025 Arduino PT2D Project
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from streaming_server import StreamingServer
from mosquito_detector import MosquitoDetector
from mosquito_tracker import MosquitoTracker
from pt2d_controller import PT2DController
from depth_estimator import DepthEstimator
from config import (DEFAULT_CONFIDENCE_THRESHOLD, DEFAULT_IMGSZ,
                   DEFAULT_DEVICE_IP, DEFAULT_EXTERNAL_URL)
import cv2
import numpy as np
import sys
import time
from pathlib import Path


class StreamingTrackingSystem:
    """æ•´åˆçš„èšŠå­è¿½è¹¤èˆ‡ä¸²æµç³»çµ±"""

    def __init__(self,
                 arduino_port: str = '/dev/ttyUSB0',
                 camera_id: int = 0,
                 model_path: str = "models/mosquito",
                 http_port: int = 5000,
                 dual_camera: bool = True,
                 stream_mode: str = "single",
                 save_samples: bool = True,
                 sample_conf_range: tuple = (0.35, 0.65),
                 enable_depth: bool = True):
        """
        åˆå§‹åŒ–å®Œæ•´ç³»çµ±

        Args:
            arduino_port: Arduino ä¸²å£
            camera_id: æ”åƒé ­ ID
            model_path: AI æ¨¡å‹è·¯å¾‘
            http_port: HTTP ä¸²æµç«¯å£
            dual_camera: æ˜¯å¦ç‚ºé›™ç›®æ”åƒé ­
            stream_mode: ä¸²æµæ¨¡å¼ ("side_by_side", "single", "dual_stream")
            save_samples: æ˜¯å¦å„²å­˜ä¸­ç­‰ä¿¡å¿ƒåº¦æ¨£æœ¬
            sample_conf_range: æ¨£æœ¬ä¿¡å¿ƒåº¦ç¯„åœ (min, max)
            enable_depth: æ˜¯å¦å•Ÿç”¨æ·±åº¦ä¼°è¨ˆ
        """
        print("=" * 60)
        print("ğŸ¦Ÿ èšŠå­è¿½è¹¤ç³»çµ± + æ‰‹æ©Ÿä¸²æµæ•´åˆå•Ÿå‹•")
        print("=" * 60)
        print()

        # ç³»çµ±é…ç½®
        self.dual_camera = dual_camera
        self.stream_mode = stream_mode
        self.camera_id = camera_id
        self.enable_depth = enable_depth and dual_camera  # æ·±åº¦ä¼°è¨ˆéœ€è¦é›™ç›®æ”åƒé ­

        # çµ±è¨ˆè³‡è¨Š
        self.stats = {
            'total_frames': 0,
            'detections': 0,
            'tracking_active': False,
            'samples_saved': 0,
            'start_time': time.time()
        }

        # 1. åˆå§‹åŒ– AI æª¢æ¸¬å™¨ï¼ˆåªå‰µå»ºä¸€æ¬¡ï¼ï¼‰
        print("[1/5] åˆå§‹åŒ– AI æª¢æ¸¬å™¨...")
        self.detector = MosquitoDetector(
            model_path=model_path,
            confidence_threshold=DEFAULT_CONFIDENCE_THRESHOLD,
            imgsz=DEFAULT_IMGSZ,
            save_uncertain_samples=save_samples,
            uncertain_conf_range=sample_conf_range,
            save_dir="uncertain_samples",
            max_disk_usage_percent=20.0,
            save_annotations=True,
            save_full_frame=False
        )
        print(f"      âœ“ ä½¿ç”¨ {self.detector.backend.upper()} å¾Œç«¯")
        if save_samples:
            print(f"      âœ“ æ¨£æœ¬å„²å­˜å·²å•Ÿç”¨ (ä¿¡å¿ƒåº¦ {sample_conf_range[0]}-{sample_conf_range[1]})")

        # 2. åˆå§‹åŒ–é›²å°æ§åˆ¶å™¨
        print("[2/5] åˆå§‹åŒ–é›²å°æ§åˆ¶å™¨...")
        try:
            self.pt_controller = PT2DController(arduino_port)
            if self.pt_controller.is_connected:
                print(f"      âœ“ Arduino å·²é€£æ¥ ({arduino_port})")
                self.has_pt = True
            else:
                print(f"      âš  ç„¡æ³•é€£æ¥ Arduinoï¼Œåƒ…é‹è¡Œæª¢æ¸¬æ¨¡å¼")
                self.has_pt = False
        except Exception as e:
            print(f"      âš  é›²å°åˆå§‹åŒ–å¤±æ•—: {e}")
            self.has_pt = False
            self.pt_controller = None

        # 3. åˆå§‹åŒ–è¿½è¹¤å™¨
        print("[3/5] åˆå§‹åŒ–è¿½è¹¤å™¨...")
        if self.has_pt:
            self.tracker = MosquitoTracker(
                detector=self.detector,
                pt_controller=self.pt_controller
            )
            print(f"      âœ“ è¿½è¹¤å™¨å·²å°±ç·’")
        else:
            self.tracker = None
            print(f"      âš  è¿½è¹¤å™¨æœªå•Ÿç”¨ï¼ˆéœ€è¦é›²å°é€£æ¥ï¼‰")

        # 4. åˆå§‹åŒ–æ·±åº¦ä¼°è¨ˆå™¨ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        print("[4/6] åˆå§‹åŒ–æ·±åº¦ä¼°è¨ˆå™¨...")
        if self.enable_depth:
            self.depth_estimator = DepthEstimator(
                focal_length=3.0,        # é¡é ­ç„¦è· 3.0mm
                baseline=120.0,          # é›™ç›®åŸºç·š 12cm
                image_width=1920,        # å–®çœ¼è§£æåº¦
                sensor_width=5.0         # æ„Ÿå…‰å…ƒä»¶å¯¬åº¦
            )
            print(f"      âœ“ æ·±åº¦ä¼°è¨ˆå·²å•Ÿç”¨ï¼ˆæ¸¬è·ç¯„åœ: 0.5-5mï¼‰")
        else:
            self.depth_estimator = None
            print(f"      âš  æ·±åº¦ä¼°è¨ˆæœªå•Ÿç”¨ï¼ˆéœ€è¦é›™ç›®æ”åƒé ­ï¼‰")

        # 5. åˆå§‹åŒ–ä¸²æµä¼ºæœå™¨
        print("[5/6] åˆå§‹åŒ–ä¸²æµä¼ºæœå™¨...")
        self.server = StreamingServer(http_port=http_port, fps=30)
        self.server.run(threaded=True)
        print(f"      âœ“ ä¸²æµä¼ºæœå™¨å·²å•Ÿå‹• (ç«¯å£ {http_port})")

        # é›™ä¸²æµæ¨¡å¼ï¼ˆåƒ…åœ¨ dual_stream æ¨¡å¼ï¼‰
        self.server_right = None
        if stream_mode == "dual_stream" and dual_camera:
            self.server_right = StreamingServer(http_port=http_port + 1, fps=30)
            self.server_right.run(threaded=True)
            print(f"      âœ“ å³å´ä¸²æµå·²å•Ÿå‹• (ç«¯å£ {http_port + 1})")

        print()
        print("=" * 60)
        print("ğŸ‰ ç³»çµ±å·²å®Œå…¨å•Ÿå‹•ï¼")
        print("=" * 60)
        print()
        # ç”Ÿæˆè¨ªå•åœ°å€
        device_ip = DEFAULT_DEVICE_IP or "[ä½ çš„IP]"
        local_url = f"http://{device_ip}:{http_port}"
        print(f"ğŸ“± æœ¬æ©Ÿè¨ªå•: {local_url}")

        # å¦‚æœè¨­ç½®äº†å¤–éƒ¨ URLï¼Œé¡¯ç¤ºå¤–éƒ¨è¨ªå•æ–¹å¼
        if DEFAULT_EXTERNAL_URL:
            print(f"ğŸŒ é ç«¯è¨ªå•: {DEFAULT_EXTERNAL_URL}")

        if self.server_right:
            right_url = f"http://{device_ip}:{http_port + 1}"
            print(f"ğŸ“± å³å´è¦–è§’ï¼ˆæœ¬æ©Ÿï¼‰: {right_url}")
        print()
        print("â„¹ï¸  ç³»çµ±é…ç½®:")
        print(f"   - AI æª¢æ¸¬: âœ“ å•Ÿç”¨ ({self.detector.backend.upper()})")
        print(f"   - é›²å°è¿½è¹¤: {'âœ“ å•Ÿç”¨' if self.has_pt else 'âœ— åœç”¨'}")
        print(f"   - é›·å°„æ¨™è¨˜: {'âœ“ å•Ÿç”¨' if self.has_laser else 'âœ— åœç”¨'}")
        print(f"   - æ·±åº¦ä¼°è¨ˆ: {'âœ“ å•Ÿç”¨' if self.enable_depth else 'âœ— åœç”¨'}")
        print(f"   - æ¨£æœ¬å„²å­˜: {'âœ“ å•Ÿç”¨' if save_samples else 'âœ— åœç”¨'}")
        print(f"   - é›™ç›®æ”åƒé ­: {'âœ“ å•Ÿç”¨' if dual_camera else 'âœ— åœç”¨'}")
        print(f"   - ä¸²æµæ¨¡å¼: {stream_mode}")
        print()
        print("âš¡ æ€§èƒ½èªªæ˜:")
        print(f"   - AI è² è¼‰: æ¯å¹€åªåŸ·è¡Œä¸€æ¬¡æª¢æ¸¬")
        print(f"   - è¨˜æ†¶é«”: å–®ä¸€æª¢æ¸¬å™¨å¯¦ä¾‹")
        print(f"   - CPU: æœ€å„ªåŒ–åˆ©ç”¨")
        print()
        print("æŒ‰éµæ“ä½œ:")
        print("   'q' - é€€å‡ºç³»çµ±")
        print("   't' - åˆ‡æ›è¿½è¹¤æ¨¡å¼")
        print("   's' - å„²å­˜æˆªåœ–")
        print("   'l' - åˆ‡æ›é›·å°„" + (" (å·²å•Ÿç”¨)" if self.has_laser else " (æœªå•Ÿç”¨)"))
        print("   'h' - é›²å°æ­¸ä½")
        print()

    def process_frame(self, frame: np.ndarray) -> np.ndarray:
        """
        è™•ç†å–®å¹€å½±åƒï¼ˆAI æª¢æ¸¬ + è¿½è¹¤ + æ¨™è¨»ï¼‰

        âš ï¸ é‡è¦ï¼šæ­¤å‡½æ•¸æ¯å¹€åªèª¿ç”¨ä¸€æ¬¡ AI æª¢æ¸¬ï¼Œä¸æœƒé‡è¤‡ï¼
        """
        self.stats['total_frames'] += 1

        # åˆ†é›¢å·¦å³ç•«é¢ï¼ˆå¦‚æœæ˜¯é›™ç›®ï¼‰
        if self.dual_camera:
            height, width = frame.shape[:2]
            mid = width // 2
            left_frame = frame[:, :mid]
            right_frame = frame[:, mid:]
        else:
            left_frame = frame
            right_frame = None

        # âš¡ AI æª¢æ¸¬ï¼ˆæ¯å¹€åªåŸ·è¡Œä¸€æ¬¡ï¼ï¼‰
        detections, result_left = self.detector.detect(left_frame)

        # è¨˜éŒ„æª¢æ¸¬æ•¸é‡
        if detections:
            self.stats['detections'] += len(detections)
            
            # ğŸ¯ æ·±åº¦ä¼°è¨ˆï¼ˆå¦‚æœå•Ÿç”¨ä¸”æœ‰å³çœ¼å½±åƒï¼‰
            if self.depth_estimator and right_frame is not None:
                for detection in detections:
                    bbox = detection.get('bbox')
                    if bbox:
                        x1, y1, x2, y2 = bbox
                        # ä¼°è¨ˆæ·±åº¦
                        depth_info = self.depth_estimator.estimate_depth_for_detection(
                            left_frame, right_frame, (x1, y1, x2, y2)
                        )
                        if depth_info:
                            detection['depth'] = depth_info['depth']
                            detection['distance_cm'] = depth_info['distance_cm']

        # è¿½è¹¤æ§åˆ¶ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        if self.tracker and detections:
            self.tracker.update(detections)
            self.stats['tracking_active'] = True
        else:
            self.stats['tracking_active'] = False

        # ç¹ªè£½ AI æ¨™è¨»ï¼ˆåŒ…å«æ·±åº¦è³‡è¨Šï¼‰
        result_left = self._draw_detections_with_depth(result_left, detections)

        # æ·»åŠ ç³»çµ±è³‡è¨Š
        self._draw_system_info(result_left, detections)

        # æ ¹æ“šä¸²æµæ¨¡å¼çµ„åˆç•«é¢
        if self.stream_mode == "side_by_side" and right_frame is not None:
            # ä¸¦æ’é¡¯ç¤º
            cv2.putText(right_frame, "Original (Right)", (10, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
            combined = np.hstack([result_left, right_frame])
            # æ·»åŠ åˆ†éš”ç·š
            mid = combined.shape[1] // 2
            cv2.line(combined, (mid, 0), (mid, combined.shape[0]),
                    (0, 255, 255), 2)
            return combined

        elif self.stream_mode == "dual_stream" and right_frame is not None:
            # é›™ä¸²æµæ¨¡å¼
            cv2.putText(right_frame, "Original (Right)", (10, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
            return result_left, right_frame

        else:
            # å–®ä¸€è¦–è§’
            return result_left

    def _draw_detections_with_depth(self, frame: np.ndarray, detections: list) -> np.ndarray:
        """
        ç¹ªè£½æª¢æ¸¬çµæœï¼ˆåŒ…å«æ·±åº¦è³‡è¨Šï¼‰

        Args:
            frame: å½±åƒå¹€
            detections: æª¢æ¸¬çµæœåˆ—è¡¨

        Returns:
            æ¨™è¨»å¾Œçš„å½±åƒ
        """
        # å…ˆç¹ªè£½åŸºæœ¬æª¢æ¸¬æ¡†
        frame = self.detector.draw_detections(frame, detections)

        # å¦‚æœå•Ÿç”¨æ·±åº¦ä¼°è¨ˆï¼Œæ·»åŠ æ·±åº¦è³‡è¨Š
        if self.depth_estimator and detections:
            for detection in detections:
                bbox = detection.get('bbox')
                depth = detection.get('depth')
                distance_cm = detection.get('distance_cm')

                if bbox and distance_cm:
                    x1, y1, x2, y2 = bbox
                    
                    # åœ¨æª¢æ¸¬æ¡†ä¸‹æ–¹é¡¯ç¤ºè·é›¢è³‡è¨Š
                    distance_text = f"{distance_cm:.1f}cm"
                    text_size = cv2.getTextSize(distance_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
                    text_x = x1
                    text_y = y2 + 25
                    
                    # ç¹ªè£½èƒŒæ™¯
                    cv2.rectangle(frame,
                                (text_x, text_y - text_size[1] - 5),
                                (text_x + text_size[0] + 5, text_y + 5),
                                (0, 0, 0), -1)
                    
                    # ç¹ªè£½è·é›¢æ–‡å­—ï¼ˆæ©™è‰²ï¼‰
                    cv2.putText(frame, distance_text,
                              (text_x + 2, text_y),
                              cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 165, 255), 2)

        return frame

    def _draw_system_info(self, frame: np.ndarray, detections: list):
        """åœ¨ç•«é¢ä¸Šç¹ªè£½ç³»çµ±è³‡è¨Š"""
        y_pos = 30
        line_height = 35

        # æ¨™é¡Œ
        cv2.putText(frame, "AI èšŠå­è¿½è¹¤", (10, y_pos),
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        y_pos += line_height

        # æª¢æ¸¬æ•¸é‡
        cv2.putText(frame, f"æª¢æ¸¬æ•¸: {len(detections)}", (10, y_pos),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        y_pos += line_height

        # è¿½è¹¤ç‹€æ…‹
        tracking_color = (0, 255, 0) if self.stats['tracking_active'] else (128, 128, 128)
        tracking_text = "è¿½è¹¤ä¸­" if self.stats['tracking_active'] else "å¾…å‘½"
        cv2.putText(frame, f"ç‹€æ…‹: {tracking_text}", (10, y_pos),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, tracking_color, 2)
        y_pos += line_height

        # FPS
        elapsed = time.time() - self.stats['start_time']
        fps = self.stats['total_frames'] / elapsed if elapsed > 0 else 0
        cv2.putText(frame, f"å¹€ç‡: {fps:.1f}", (10, y_pos),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)

        # ä¸²æµè³‡è¨Šï¼ˆå³ä¸Šè§’ï¼‰
        cv2.putText(frame, f"å®¢æˆ·ç«¯: {self.server.stats['clients']}",
                   (frame.shape[1] - 200, 30),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

    def run(self):
        """é‹è¡Œä¸»å¾ªç’°"""
        # é–‹å•Ÿæ”åƒé ­
        cap = cv2.VideoCapture(self.camera_id)

        if self.dual_camera:
            cap.set(cv2.CAP_PROP_FRAME_WIDTH, 3840)
            cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)
            cap.set(cv2.CAP_PROP_FPS, 60)

        if not cap.isOpened():
            print("âœ— ç„¡æ³•é–‹å•Ÿæ”åƒé ­")
            return

        print(f"âœ“ æ”åƒé ­å·²é–‹å•Ÿ (ID: {self.camera_id})")
        print()

        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    print("âœ— ç„¡æ³•è®€å–å½±åƒ")
                    break

                # âš¡ è™•ç†å½±åƒï¼ˆæ¯å¹€åªåŸ·è¡Œä¸€æ¬¡ AI æª¢æ¸¬ï¼‰
                result = self.process_frame(frame)

                # æ›´æ–°ä¸²æµ
                if self.stream_mode == "dual_stream" and isinstance(result, tuple):
                    # é›™ä¸²æµæ¨¡å¼
                    self.server.update_frame(result[0])
                    if self.server_right:
                        self.server_right.update_frame(result[1])
                    display = np.hstack([
                        cv2.resize(result[0], (960, 540)),
                        cv2.resize(result[1], (960, 540))
                    ])
                else:
                    # å–®ä¸€ä¸²æµ
                    self.server.update_frame(result)
                    display = result

                # æœ¬åœ°é è¦½
                cv2.imshow('èšŠå­è¿½è¹¤ç³»çµ±', display)

                # éµç›¤æ§åˆ¶
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    print("\né€€å‡ºä¸­...")
                    break
                elif key == ord('t'):
                    if self.tracker:
                        print(f"è¿½è¹¤æ¨¡å¼: {'å•Ÿç”¨' if not self.stats['tracking_active'] else 'åœç”¨'}")
                elif key == ord('s'):
                    filename = f"capture_{int(time.time())}.jpg"
                    cv2.imwrite(filename, display)
                    print(f"å·²å„²å­˜: {filename}")
                elif key == ord('h'):
                    if self.pt_controller:
                        self.pt_controller.home()
                        print("é›²å°æ­¸ä½ä¸­...")
                    else:
                        print("é›²å°æœªé€£æ¥")

        except KeyboardInterrupt:
            print("\n\nç”¨æˆ¶ä¸­æ–· (Ctrl+C)")

        finally:
            # æ¸…ç†è³‡æº
            cap.release()
            cv2.destroyAllWindows()
            if self.pt_controller:
                self.pt_controller.close()

            # é¡¯ç¤ºçµ±è¨ˆ
            print()
            print("=" * 60)
            print("ç³»çµ±çµ±è¨ˆ")
            print("=" * 60)
            print(f"ç¸½å¹€æ•¸: {self.stats['total_frames']}")
            print(f"ç¸½æª¢æ¸¬: {self.stats['detections']}")
            if hasattr(self.detector, 'saved_sample_count'):
                print(f"å·²å„²å­˜æ¨£æœ¬: {self.detector.saved_sample_count}")
            elapsed = time.time() - self.stats['start_time']
            print(f"é‹è¡Œæ™‚é–“: {elapsed:.1f} ç§’")
            print(f"å¹³å‡ FPS: {self.stats['total_frames'] / elapsed:.1f}")
            print()


def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    print()
    print("=" * 60)
    print("ğŸ¦Ÿ èšŠå­è¿½è¹¤ç³»çµ± + æ‰‹æ©Ÿä¸²æµ")
    print("=" * 60)
    print()

    # æª¢æŸ¥æ˜¯å¦ä½¿ç”¨ Windowsï¼ˆå¯èƒ½éœ€è¦èª¿æ•´ä¸²å£ï¼‰
    default_port = 'COM3' if sys.platform.startswith('win') else '/dev/ttyUSB0'

    # ç°¡å–®é…ç½®
    print("ç³»çµ±é…ç½®:")
    print()
    arduino_port = input(f"Arduino ä¸²å£ [{default_port}]: ").strip() or default_port

    print()
    print("ä¸²æµæ¨¡å¼:")
    print("1. ä¸¦æ’é¡¯ç¤º - å·¦å´ AI æ¨™è¨» + å³å´åŸå§‹")
    print("2. å–®ä¸€è¦–è§’ï¼ˆé è¨­ï¼‰- åƒ… AI æ¨™è¨»")
    print("3. ç¨ç«‹é›™ä¸²æµ - å·¦å³åˆ†åˆ¥ä¸²æµ")
    mode_choice = input("é¸æ“‡æ¨¡å¼ [2]: ").strip() or "2"

    mode_map = {
        "1": "side_by_side",
        "2": "single",
        "3": "dual_stream"
    }
    stream_mode = mode_map.get(mode_choice, "single")

    print()

    # å‰µå»ºä¸¦é‹è¡Œç³»çµ±
    system = StreamingTrackingSystem(
        arduino_port=arduino_port,
        camera_id=0,
        model_path="models/mosquito",
        http_port=5000,
        dual_camera=True,
        stream_mode=stream_mode
    )

    system.run()


if __name__ == "__main__":
    main()
